{"name":"Administration-Rhetoric-of-Terror","tagline":"Tracing the Rhetoric of the \"War on Terror\"","body":"#War Rhetoric in Non-Governmental Discourse\r\n\r\nThis project examines the rhetoric surrounding the \"War on Terror\" and traces the extent to which that rhetoric has been adopted in non-governmental discourse. By mining the texts produced by the premiers of United States and the United Kingdoms (from the Bush and Blair administrations to the present), we will identify keywords that define each realm of communication, focusing on terms “terrorism,” \"security,\" \"protection\" and their stem words, which have defined the many American wars of the century so far. We will mark the relative frequency of these terms, the terms that are covariant with them, and their context within the larger discourse. This rhetorical analysis will happen in two parallel parts. In the first part, Phillip R. Polefrone will show how the textual material was selected and obtained using Python-driven web scraping techniques, then perform quantitative analysis on the selected corpus using TAPoR, Voyant Tools and NVivo to characterize the rhetoric of the Bush administration versus the Obama administration. In the second part, May Hany El Maraashly will trace the way the \"War on Terror\" was discussed in social media during the same period, considering the terms used and the broader trends in the discourse. Our expectation is that these examinations will show a parallelism between the terms established by the American government and the discussion in the public sphere.\r\n\r\n##Phillip's Part\r\n\r\n###1. Gathering the Data\r\n\r\nThe first step in analyzing the speeches of the Bush and Obama administrations was to select the data and obtain plaintext versions of the speeches. In order to limit the dataset to major speeches by the sitting premier, the site *presidentialrhetoric.com* was used as a datasource. This limitation was voluntarily assumed for two reasons. The first is that by limiting the dataset to *major* speeches as *presidentialrhetoric.com* does, we ensure that we are working with the clearest articulation of rhetorical policy, thereby addressing the intent of certain usages and word pairings more directly. We determined that press releases and announcements from, for example, Jay Carney, the standing Whitehouse Press Secretary, would have a lesser degree of rhetorical intentionality because of their comparative spontaneity. As such, examining these statements for their rhetorical content would be less ingenuous and potentially more opportunistic. The second reason for limiting our selection to the content represented on *presidentialrhetoric.com* was the ease of scraping the plaintext versions. While scraping the contents of this site presented its challenges, the content was clearly organized and friendly to scraping. In contrast, the press section of *whitehouse.gov* made scraping difficult because of its paging mechanism, which spoiled all of our spiders (formulated using Scrapy). In a less modest iteration of this project, other methods of scraping would be investigated. For these reasons we obtained the HTML versions of the relevant pages from *presidentialrhetoric.com* and continued with our cleaning.\r\n\r\nThe next step was to extract the plaintext of the speeches from the collected HTML data. In this step, we used Python and the Beautiful Soup module to parse the raw HTML. We began by identifying the location of the relevant data in the HTML structure, and found that by checking for HTML tags with certain attributes we could isolate the text we wanted. The following is a representative example from the raw HTML:\r\n\r\n`````\r\n<p align=\"left\" class=\"style2\">Mrs. Ford, the Ford family; distinguished guests, including our Presidents and First Ladies; and our fellow citizens:</p>\r\n\r\n<p align=\"left\" class=\"style2\">We are here today to say goodbye to a great man. Gerald Ford was born and reared in the American heartland. He belonged to a generation that measured men by their honesty and their courage. He grew to manhood under the roof of a loving mother and father -- and when times were tough, he took part-time jobs to help them out. In President Ford, the world saw the best of America -- and America found a man whose character and leadership would bring calm and healing to one of the most divisive moments in our nation's history.</p>\r\n`````\r\n\r\nBy specifying the \"align\" and \"class\" attributes, we are able to extract this text using Beautiful Soup. Not all pages were identical, however, so we cycled through the several different formats used by *presidentialrhetoric.com* using an *if* statements to check if the text field was not yet populated (meaning that the relevant field had not been identified, and thus a different set of attributes would have to be specified). Using this structure, we arrived at the following code, which is compilable as \"convertall.py\" in the attached directory:\r\n\r\n<pre><code>\r\n#!/usr/bin/env python\r\n\r\nfrom bs4 import BeautifulSoup\r\nfrom types import *\r\nimport os\r\n\r\ndef checkfile(filename):\r\n    content = open(\"TextOnly/\"+filename,\"r+\")\r\n    print content.read()\r\n\r\ndef pulltext(filename):    \r\n    text = []\r\n    soup = BeautifulSoup(open(\"www.presidentialrhetoric.com/speeches/\"+filename,\"r\"))\r\n    textversion = open(\"TextOnly/\"+filename+\".txt\",\"w\")\r\n    for p in soup.find_all('p'):\r\n        if p.attrs == {'align': 'left', 'class': ['style2']}:\r\n            string = p.string\r\n            if type(string) is not NoneType:\r\n                try:\r\n                    string.encode('ascii')\r\n                    text.append(string)\r\n                except UnicodeEncodeError:\r\n                    pass\r\n            else:\r\n                pass\r\n    for p in soup.find_all('span'):\r\n        if p.attrs == {'class': ['style2']}:\r\n            string = p.string\r\n            if type(string) is not NoneType:\r\n                try:\r\n                    string.encode('ascii')\r\n                    text.append(string)\r\n                except UnicodeEncodeError:\r\n                    pass\r\n    if text == []:\r\n        for p in soup.find_all('p'):\r\n            string = p.string\r\n            if type(string)is not NoneType:\r\n                try:\r\n                    string.encode('ascii')\r\n                    text.append(string)\r\n                except UnicodeEncodeError:\r\n                    pass\r\n    \"\"\"for l in text:\r\n        if type(l) is NoneType:\r\n            del text[text.index(l)]\r\n            return text\"\"\"\r\n    textversion.write(''.join(text))\r\n    textversion.close()\r\n\r\ndef wordlist(fname):\r\n    file = open('TextOnly/'+str(fname),\"r+\")\r\n    if fname != '.DS_Store':\r\n        fcontent = open('TextOnly/'+str(fname),\"r+\")\r\n        text = fcontent.read()\r\n        fstr = text.split()\r\n        for l in fstr:\r\n            if str(l) == '(laughter.)' or str(l) == '(laughter)' or str(l) == '(Laughter.)' or str(l) == '(Laughter)' or str(l) == '(applause.)' or str(l) == '(applause)' or str(l) == '(Applause.)' or str(l) == '(Applause)':\r\n                fstr.remove(l)\r\n    return fstr\r\n\r\nfor fname in os.listdir('www.presidentialrhetoric.com/speeches'):\r\n    if fname == \".DS_Store\":\r\n        pass\r\n    else:\r\n        try:\r\n            pulltext(fname)\r\n        except Exception:\r\n            print 'error' + fname\r\n</code></pre>\r\n\r\nHere pulltext() is the main function, while wordlist() is a secondary function meant as the beginning of a concordance---by splitting the speech into an iterable of the words it contains, it would be possible to check instances of identical list items and construct a concordance of the collected speeches. Other existing tools (discussed below) made this concordance unnecessary, but the code was preserved for possible use in cleaning (splitting a speech into words in an iterable and recombining it is an easy way to get rid of white space). \r\n\r\nThis code, when run from the root project directory, creates a folder with the plaintext versions. These files were named by date when downloaded from *presidentialrhetoric.com*, but for preparation for use in Voyant Tools the format needed to be revised from MMDDYY to YYMMDD (so that they would appear in chronological order). The following code was written to that effect, which can be run as fnamedater.py from the root project directory:\r\n\r\n<pre><code>\r\nfrom bs4 import BeautifulSoup\r\nfrom types import *\r\nimport os\r\n\r\ndef checkfile(filename):\r\n    content = open(\"TextOnly/\"+filename,\"r+\")\r\n    print content.read()\r\n    \r\nfor fname in os.listdir('TextOnly'):\r\n    if fname != \".DS_Store\":\r\n        file = open('TextOnly/'+fname, 'r')\r\n        text = file.read()\r\n        date = []\r\n        year = fname[6:8]\r\n        month = fname[0:2]\r\n        day = fname[3:5]\r\n        date.append(year)\r\n        date.append(month)\r\n        date.append(day)\r\n        newfname = ''.join(date)\r\n        newfile = open('TextOnlyDated/'+newfname+'.txt','w')\r\n        newfile.write(text)\r\n        newfile.close()\r\n        file.close()\r\n</code></pre>\r\n\r\nThis creates a new set of files in a new directory (\"TextOnlyDated\"), which can be compressed into a single file and uploaded to Voyant Tools or NVivo. \r\n\r\nThis was the final stage of preparation for the text of the speeches before analysis was able to begin.\r\n\r\n###2. Analyzing the Data\r\n\r\nOur analysis was framed as a comparison between the Bush and Obama administrations. The purpose of this choice is to see to what extent change in administration corresponds to change in rhetorical strategies. From here, the examination occurred in three phases. First, we compared word frequencies in the use of \"terror\" and its stem words. Next, we compared the words that are covariant with \"terror\" and its stem words in each administration. Finally, we compared the contextual environment of these occurrences (prepositions, full stops, and so forth).\r\n\r\n####2.1 Frequency\r\n\r\n####2.2 Covariants\r\n\r\n####2.3 Context\r\n\r\n###3. Conclusions\r\n\r\n\r\n ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}